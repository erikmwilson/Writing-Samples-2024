=======
Compute
=======

This chapter discusses how to manage Compute (nova) for RPCO.


Adding a Compute node
~~~~~~~~~~~~~~~~~~~~~

After installation, depending on the OpenStack environment, additional
compute hosts can provide more resources for the cluster. The following
procedure details how to add a compute host to an operational cluster.

  .. warning::

     Make sure you back up your current OpenStack environment
     before adding any new nodes. See :ref:`backing-up-internal` for more
     information.

#. Configure the host as a target host. See the `Rackspace Private
   Cloud Installation Guide
   <https://pages.github.rackspace.com/rpc-internal/docs-rpc/index.html>`_
   chapter on Target Hosts for more information.

#. Edit the ``/rpcd/playbooks/openstack_user_config.yml`` file and
   add the host to the ``compute_hosts`` stanza.

   .. code-block:: ini

      compute_hosts:
      [...]
      NEW_compute<node-ID>:
        ip: 10.17.136.32
      NEW_compute<node-ID>:
        ip: 10.17.136.33

   .. note::

      Modify the ``used_ips`` stanza if needed.

#. Run the add host playbook while logged in to the deployment
   host.

   .. code-block:: console

      # cd /opt/rpc-openstack/openstack-ansible/playbooks

#. Update the inventory to add new hosts. Make sure new rsyslog
   container names are updated. Send the updated results to ``dev/null``.

   .. code-block:: console

      # /opt/rpc-openstack/openstack-ansible/playbooks/inventory/dynamic_inventory.py > /dev/null

#. Create the ``/root/add_host.limit`` file, which contains all new node
   host names.

   .. code-block:: console

      # /opt/rpc-openstack/openstack-ansible/scripts/inventory-manage.py \
        -f /opt/rpc-openstack/openstack-ansible/playbooks/inventory/dynamic_inventory.py \
        -l |awk -F\| '/<NEW COMPUTE NODE>/ {print $2}' |sort -u | tee /root/add_host.limit

#. Run the appropriate playbooks.

   .. code-block:: console

      # cd /opt/rpc-openstack/openstack-ansible/playbooks
      # openstack-ansible setup-hosts.yml --limit @/root/add_host.limit
      # ansible nova_all -m setup -a 'filter=ansible_local gather_subset="!all"'
      # openstack-ansible setup-openstack.yml --skip-tags nova-key-distribute --limit @/root/add_host.limit
      # openstack-ansible setup-openstack.yml --tags nova-key --limit compute_hosts

#. Run the rpc-support playbooks.

   .. code-block:: console

      # ( cd /opt/rpc-openstack/rpcd/playbooks ; openstack-ansible rpc-support.yml )

#. Generate a new impersonation token, and add that token after the
   `maas_auth_token` variable in the ``user_rpco_variables_overrides.yml``
   file.

   .. code-block:: ini

      - /etc/openstack_deploy/user_rpco_variables_overrides.yml
      - maas_auth_token: <*new_auth_token*>

#. Run the MaaS playbook on the deployment host.

   .. code-block:: console

      # ( cd /opt/rpc-openstack/rpcd/playbooks ; openstack-ansible \
          setup-maas.yml --limit @/root/add_host.limit )

.. #. Run the following commands to add the host. Replace
..    ``NEW_HOST_NAME`` with the name of the new host.

..    .. code-block:: console

..      $ cd /opt/openstack-ansible/playbooks
..      $ openstack-ansible setup-everything.yml \
..      rsyslog-config.yml --limit NEW_HOST_NAME
..      .. TODO(JR) Confirm this step can be removed.

Adding a new infrastructure node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While three infrastructure hosts are recommended, if further hosts are
needed in an environment, it is possible to create additional nodes.

  .. warning::

     Make sure you back up your current OpenStack environment
     before adding any new nodes. See :ref:`backing-up-internal` for more
     information.

#. Add the node to the ``infra_hosts`` stanza of the
   ``/etc/openstack_deploy/openstack_user_config.yml``

   .. code-block:: ini

      infra_hosts:
      [...]
      NEW_infra<node-ID>:
        ip: 10.17.136.32
      NEW_infra<node-ID>:
        ip: 10.17.136.33

#. Run the ``add_host`` playbook on the deployment host.

   .. code-block:: console

      # cd /opt/rpc-openstack/openstack-ansible/playbooks

#. Update the inventory to add new hosts. Make sure new rsyslog
   container names are updated. Send the updated results to ``dev/null``.

   .. code-block:: console

      # /opt/rpc-openstack/openstack-ansible/playbooks/inventory/dynamic_inventory.py > /dev/null

#. Create the ``/root/add_host.limit`` file, which contains all new node
   host names.

   .. code-block:: console

      # /opt/rpc-openstack/openstack-ansible/scripts/inventory-manage.py \
        -f /opt/rpc-openstack/openstack-ansible/playbooks/inventory/dynamic_inventory.py \
        -l |awk -F\| '/<NEW COMPUTE NODE>/ {print $2}' |sort -u | tee /root/add_host.limit

#. Run the ``setup-everything.yml`` playbook with the
   ``--limit`` argument.

  .. warning::

     Do not run the ``setup-everything.yml`` playbook
     without the :option:``--limit`` argument. Without
     :option:``--limit``, the playbook will restart all containers
     inside your RPCO environment.

  .. code-block:: console

     # openstack-ansible setup-everything.yml --limit @/root/add_host.limit
     # openstack-ansible --tags=openstack-host-hostfile setup-hosts.yml

#. Run the compute playbook on the deployment host.

   .. code-block:: console

      # openstack-ansible os-nova-install.yml --skip-tags nova-key-distribute \
        --limit @/root/add_host.limit
      # openstack-ansible os-nova-install.yml \
        --tags nova-key-create,nova-key-distribute

#. Run the Neutron agent playbook on the deployment host.

   .. code-block:: console

      # openstack-ansible os-neutron-install.yml \
        --limit @/root/add_host.limit

#. Run the rpc-support playbooks.

   .. code-block:: console

      # ( cd /opt/rpc-openstack/rpcd/playbooks ; openstack-ansible rpc-support.yml )

#. Generate a new impersonation token, and add that token after the
   `maas_auth_token` variable in the ``user_rpco_variables_overrides.yml``
   file.

   .. code-block:: ini

      - Update maas_auth_token /etc/openstack_deploy/user_rpco_variables_overrides.yml

#. Run the MaaS playbook on the deployment host.

   .. code-block:: console

      # ( cd /opt/rpc-openstack/rpcd/playbooks ; openstack-ansible \
          setup-maas.yml --limit @/root/add_host.limit )

Testing new nodes
~~~~~~~~~~~~~~~~~

After creating a new node, test that the node runs correctly by
launching a new instance. Ensure that the new node can respond to
a networking connection test through the :command:`ping` command.
Log in to Monitoring.rackspace.net, and verify that the monitors
return a green signal for the new node.

Recovering a Compute node failure
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following procedure addresses Compute node failure if shared storage
is used.

   .. note::

      If shared storage is not used, data can be copied from the
      ``/var/lib/nova/instances`` directory on the failed Compute node
      ``${FAILED_NODE}`` to another node ``${RECEIVING_NODE}``\ before
      performing the following procedure. Please note this method is
      not supported.

   #. Re-launch all instances on the failed node.

   #. Invoke the MySQL command line tool

   #. Generate a list of instance UUIDs hosted on the failed node:

      .. code-block:: console

         mysql> select uuid from instances where host = '${FAILED_NODE}' and deleted = 0;

   #. Set instances on the failed node to be hosted on a different node:

      .. code-block:: console

         mysql> update instances set host ='${RECEIVING_NODE}' where host = '${FAILED_NODE}' \
         and deleted = 0;

   #. Reboot each instance on the failed node listed in the previous query
      to regenerate the XML files:

      .. code-block:: console

         # nova reboot â€”hard $INSTANCE_UUID

      #. Find the volumes to check the instance has successfully booted and is
         at the login  :

      .. code-block:: console

         mysql> select nova.instances.uuid as instance_uuid, cinder.volumes.id \
                as voume_uuid, cinder.volumes.status, cinder.volumes.attach_status, \
                cinder.volumes.mountpoint, cinder.volumes,display_name from \
                cinder.volumes inner join nova.instances on cinder.volumes.instance_uuid=nova.instances.uuid \
                where nova.instances.host = '${FAILED_NODE}';

      #. If rows are found, detach and re-attach the volumes using the values
         listed in the previous query:

      .. code-block:: console

         # nova volume-detach $INSTANCE_UUID $VOLUME_UUID && \
           nova volume-attach $INSTANCE_UUID $VOLUME_UUID $VOLUME_MOUNTPOINT

      #. Rebuild or replace the failed node as described in `Adding a Compute
         node <compute-add-node.html>`_


Decommissioning a Compute node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Investigate the node status, and ensure that all instances stored on
   the node are removed and backed up elsewhere in the OpenStack
   environment.

#. Check the shell environment, ensuring that the OS\_\* variables are
   defined by sourcing the ``openrc`` file.

#. Secure a list of instances that need to change location.

   .. code-block:: console

      $ nova list --host {HOSTNAME} --all-tenants

   ``HOSTNAME`` is the value returned by the **nova list --host**
   command.

#. Move the instances one at a time either through **block-migrate** or
   with **nova live-migration** if using shared storage.

   .. code-block:: console

      $ nova live-migration {UUID} {HOSTNAME}

   Use the following procedure for **block-migrate** in all other cases.

   .. warning::

      The :command:``--block-migrate`` flag disperses the nova
      instance disks from a thin to a fat file.

   .. code-block:: console

      $ nova live-migration --block-migrate {UUID} {HOSTNAME}

   The ``UUID`` value is the instance ID value identified by the
   **nova host-list** in step one.

#. Stop the compute node after moving the instances.

   .. code-block:: console

      $ stop nova-compute

   .. note::

      After stopping the compute node, the `OpenStack Operations
      Guide, maintenance chapter
      <http://docs.openstack.org/openstack-ops/content/maintenance.html>`_
      has specific commands for environments running Puppet, or other
      configuration management system.

#. Shut down the compute node, perform any required maintenance, and
   restart the compute node.

   .. code-block:: console

      $ start nova-compute

#. If necessary, migrate any instances moved as a result of this process
   back to the compute node.

Accessing the NoVNC console
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The default configuration for RPCO is the NoVNC console. All new instances use
the NoVNC console.

.. note::

   Access for the NoVNC console is intended only for new installations.

.. code-block:: ini

   /etc/openstack_deploy/user_rpco_variables_overrides.yml
   nova_console_type: novnc

Convert running instances to use the ``novnc`` console type by performing a
hard reboot, which automatically applies this configuration.

.. _backing-up-internal:

Checking for recent back ups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before adding new nodes to your RPCO environment, it is possible to
confirm that a recent back up resides inside the ``holland_backups``
repository.

#. Log in to the Infra host where the Galera service creates backups.

#. Run the :command:`ls -ls` command to view the contents of the
   back up files:

   .. code-block:: console

      <node-ID>-Infra01~#: ls -ls /openstack/backup/XXXX_galera_container-XXXXXXX/holland_backups/rpc_support/
